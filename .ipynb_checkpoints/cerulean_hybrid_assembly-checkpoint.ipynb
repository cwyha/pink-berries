{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "~Intended to be used with CeruleanTools AMI~\n",
    "\n",
    "Steps: \n",
    "\n",
    "\n",
    "B] Inputs and Pre-processing:\n",
    " i) Assembled contigs from ABySS short read assembler\n",
    " ii)Mapping of Pacbio reads to ABySS contigs using BLASR\n",
    "\n",
    " i) Assembly of Illumina paired-end reads:\n",
    "   If the paired-end reads are stored in fastq format in the files `reads1.fastq` and `reads2.fastq`, then contigs may be assembled by:\n",
    "   \n",
    "```\n",
    "$ abyss-pe k=64 n=10 in='reads1.fastq reads2.fastq' name=<dataname>\n",
    "```\n",
    "\n",
    "This will generate 2 files used for inputs to Cerulean:\n",
    "```\n",
    "* <dataname>-contigs.fa    #This contains the contig sequences\n",
    "* <dataname>-contigs.dot   #This contains the graph structure\n",
    "```\n",
    "\n",
    " ii)Mapping PacBio reads to ABySS contigs using BLASR:\n",
    "   Note: sawriter and blasr are part of SMRT Analysis toolkit\n",
    "   Note: You need to set the environmental variables and path:\n",
    "   \n",
    "```\n",
    "$ export SEYMOUR_HOME=/opt/smrtanalysis/\n",
    "$ source $SEYMOUR_HOME/etc/setup.sh\n",
    "```\n",
    "   \n",
    "Suppose PacBio reads are stored in `<dataname>_pacbio.fasta`\n",
    "\n",
    "```\n",
    "$ sawriter <dataname>-contigs.fa\n",
    "$ blasr <dataname>_pacbio.fa <dataname>-contigs.fa -minMatch 10 \\\n",
    "     -minPctIdentity 70 -bestn 30 -nCandidates 30 -maxScore -500 \\\n",
    "     -nproc <numthreads> -noSplitSubreads \\\n",
    "     -out <dataname>_pacbio_contigs_mapping.fasta.m4\n",
    "```\n",
    "   \n",
    "   Make sure the fasta.m4 file generated has the following format:\n",
    "   qname tname qstrand tstrand score pctsimilarity tstart tend tlength \\\n",
    "   qstart qend qlength ncells\n",
    "   The file format may be verified by adding the option -header to blasr. \n",
    "\n",
    "C] Execute:\n",
    " Cerulean requires that all input files are in the same directory `<basedir>`:\n",
    " i)   `<basedir>/<dataname>-contigs.fa`\n",
    " ii)  `<basedir>/<dataname>-contigs.dot`\n",
    " iii) `<basedir>/<dataname>_pacbio_contigs_mapping.fasta.m4`\n",
    "\n",
    " To run:\n",
    " ```\n",
    " $ python src/Cerulean.py --dataname <dataname> --basedir <basedir> \\\n",
    " --nproc <numthreads>\n",
    " ```\n",
    " \n",
    " This will generate:\n",
    " i)  `<basedir>_cerulean.fasta`\n",
    " ii) `<basedir>_cerulean.dot`\n",
    " Note: The dot does not have same contigs as fasta, but intermediate graph.\n",
    " \n",
    " \n",
    "D] Post-processing:\n",
    " Currently Cerulean does not include consensus sequence of PacBio reads in gaps\n",
    " The gaps may be filled using PBJelly.\n",
    " ```\n",
    " $ python $JELLYPATH/fakeQuals.py <dataname>_cerulean.fasta <dataname>_cerulean.qual\n",
    " $ python $JELLYPATH/fakeQuals.py <dataname>_pacbio.fasta <dataname>_pacbio.qual\n",
    " $ cp $JELLYPATH/lambdaExample/Protocol.xml .\n",
    " $ mkdir PBJelly\n",
    " ```\n",
    " \n",
    " Modify Protocol.xml as follows:\n",
    " Set `<reference>` to `$PATH_TO_<basedir>/<dataname>_cerulean.fasta`\n",
    " Set `<outputDir>` to `$PATH_TO_<basedir>/PBJelly`\n",
    " Set `<baseDir>` to `$PATH_TO_<basedir>`\n",
    " Set `<job>` to `<dataname>_pacbio.fasta`\n",
    " Set `<blasr>` option `-nproc <numthreads>`\n",
    " \n",
    " Note: PBJelly requires that the suffix be .fasta and not .fa\n",
    " Next run PBJelly:\n",
    " \n",
    " ```\n",
    " ($ source $JELLYPATH/exportPaths.sh)\n",
    " $ python $JELLYPATH/Jelly.py <stage> Protocol.xml\n",
    " ```\n",
    " \n",
    " where <stage> has to be in the order:\n",
    " ```\n",
    " setup\n",
    " mapping\n",
    " support\n",
    " extraction\n",
    " assembly\n",
    " output\n",
    " ```\n",
    " \n",
    " The assembled contigs may be view in \n",
    " ```\n",
    " <basedir>/PBJelly/assembly/jellyOutput.fasta\n",
    " ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "*Getting Started*\n",
    "\n",
    "Start by creating a snapshot from the Pink Berries Metagenome snapshot, and starting a CeruleanTools AMI Instance with that volume loaded onto it.\n",
    "\n",
    "You'll have to make a directory for the drive and mount the drive. My metagenome data volume was at `/dev/xvdc`; I'm not sure how to tell where it will be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘/home/ubuntu/data’: File exists\n",
      "mount: mount point data does not exist\n"
     ]
    }
   ],
   "source": [
    "! mkdir ~/data\n",
    "! sudo mount /dev/xvdc data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
